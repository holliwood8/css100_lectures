{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565fd595",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58997db4-551c-4a50-973a-e8fc2a4d2df2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lecture plan\n",
    "\n",
    "- Review: **deep neural networks**.\n",
    "- Conceptual introduction to training neural networks.\n",
    "   - High-level overview of **back-propagation**.\n",
    "- Introduction to `pytorch`.\n",
    "   - A simple classification problem using `torch`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c083e4d4-9620-4327-9c05-0ab4ab7d78dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Review: what is a *neural network*? \n",
    "\n",
    "> A **neural network** is a predictive model loosely inspired by biological neural networks, which *updates weights* based on data to make better predictions.\n",
    "\n",
    "The most basic neural network has several ingredients:\n",
    "\n",
    "- An *input layer*, or $X$.\n",
    "- An *output layer*, or $Y$.\n",
    "- *Weights* $W$ (analogous to coefficients $\\beta$)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0da36eef-a9dd-4a60-8d12-f73bc5e31000",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Why do hidden layers work?\n",
    "\n",
    "Most moden neural networks have **hidden layers**.\n",
    "\n",
    "> A **hidden layer** adds new *parameters* to a neural network, which expands the space of features (and relationships) the system can learn.\n",
    "\n",
    "- Similar intuition as a **non-linear kernel trick** for SVM.\n",
    "- Also similar to **polynomial regression**.\n",
    "\n",
    "\n",
    "<img src=\"img/networks/nn3.png\" width=\"300\" alt=\"Larger model\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7606d2e8-36b3-4247-a91f-9629cb928daf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## On \"Learning\"\n",
    "\n",
    "> Neural networks **update their weights** to make better predictions; this process is sometimes called *learning*.\n",
    "\n",
    "In this section, we'll briefly discuss how neural networks update their weights, which also involves a technique called *backpropagation*. \n",
    "\n",
    "We'll focus a simple kind of network called a **feed-forward neural network**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be27d8b3-d580-470d-9825-aac110434c9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### \"Feed-forward\" networks\n",
    "\n",
    "> A **feedforward neural network (FFN)** is a neural network with no \"cycles\". Each unit connects only “forward” to units in the next layer.\n",
    "\r\n",
    "This is a feed-forward model with a single hidden layer. \n",
    "\n",
    "<img src=\"img/networks/ffn.png\" width=\"300\" alt=\"Simple FFN\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66c5b307-f352-455d-bbd8-3dfdcab6f791",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Getting *predictions* from a feed-forward model\n",
    "\n",
    "- **Step 1**: Multiply inputs $X$ by weights $W$. \n",
    "- **Step 2**: Apply some *activation function* to obtain hidden unit activations.\n",
    "- **Step 3**: Multiply hidden units by weights $U$.\n",
    "- **Step 4**: Apply *soft-max* function to obtain predictions $\\hat{Y}$.\n",
    "\n",
    "<img src=\"img/networks/ffn.png\" width=\"300\" alt=\"Simple FFN\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "050f3072-b36c-4f20-9b62-ed2c67ce9e75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Getting predictions: the equations\n",
    "\n",
    "First, we obtain hidden layer activations:\n",
    "\n",
    "$$h = \\sigma(Wx + b)$$\n",
    "\n",
    "Then, we multiply hidden layer by weights $U$.\n",
    "\n",
    "$$z = Uh + b$$\n",
    "\n",
    "Finally, we *softmax* this to obtain a **probability distribution**.\n",
    "\n",
    "$$y = softmax(z)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1b5135b-c859-4c2f-bfe3-4028f6c06d8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Different activation functions\n",
    "\n",
    "> An **activation function** maps $Wx$ through some (typically non-linear) function we call each hidden unit's \"activation\". **Non-linear activation functions** are important for making neural networks more powerful.\n",
    "\n",
    "- *Sigmoid* activation function: \n",
    "\n",
    "$$ g(z) \\ = \\ \\dfrac{1}{1 + e^{-z}}$$\n",
    "\n",
    "- *Rectified linear unit* function:\n",
    "\n",
    "$$ g(z) \\ = \\ (z)_+ $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfea0074-8342-4f15-8a3f-319ad3bb23ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Illustration of activation functions\n",
    "\n",
    "- Both the *sigmoid* and *ReLU* activation functions are non-linear.\n",
    "- These days, ReLU is more common because it is more **efficient** and tends to lead to **better performance**. \n",
    "\n",
    "<img src=\"img/networks/nn2.png\" width=\"400\" alt=\"Activation function\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8885d5d2-9194-4a34-89ac-4d67b5edee96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Check-in: random weights\n",
    "\n",
    "When we **train** a neural network, we typically start with *random weights*. What does that mean about our predictions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36fda25-7c7f-4814-aab8-4cd61fc63628",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b078ecab-0fb9-4aa0-832c-ad341e626de8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Where do the weights come from?\n",
    "\n",
    "- When we **train** a neural network, we typically start with *random weights*.\n",
    "- This means that at first, our predictions will be very wrong.\n",
    "- However, we can **adjust** those weights iteratively until we get better and better predictions.\n",
    "\n",
    "> Analogy: turning the knob in a shower until you reach the desired temperature.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "223c1d70-d7ea-4187-ba1a-cfed3aaf75b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Back-propagation, briefly explained\n",
    "\n",
    "> **Backpropagation** is a technique for *propagating* the error signal from the *output layer* back through the network to update the weights at each layer.\n",
    "\n",
    "- **Forward pass**: generate predictions based on input $X$.\n",
    "- **Compute error**: compare prediction to actual value(s).\n",
    "- **Backward pass**: propagate error signal back through network to improve predictions.\n",
    "\n",
    "<img src=\"img/networks/backprop.png\" width=\"400\" alt=\"Backpropagation\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92c9d6dd-2854-4374-af4b-cd2955cafa70",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Calculating the error\n",
    "\n",
    "- First, the **error** (or **loss**) is calculated by comparing the network's *prediction* to the actual value.\n",
    "- Conceptually, this is very similar to $MSE$ or related concepts!\n",
    "\n",
    "> Given some initial parameters $\\theta$, we can calculate the **error** by defining some loss function $J(\\theta)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91fec33-6194-4c6e-894b-dd4e4d4984d7",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Updating the parameters \n",
    "\n",
    "> **Gradient descent** is used to iteratively update the weights such that our **cost** $J(\\theta)$ is *minimized*.\n",
    "\n",
    "- We want to find the value(s) of our weight(s) $\\theta$ that minimize our cost.\n",
    "- Analogy: \"rolling down a hill\" to find the lowest point (least error).\n",
    "  - The **learning rate** determines the amount that we roll each time!\n",
    "\n",
    "<img src=\"img/networks/gradient_descent.png\" width=\"400\" alt=\"Gradient descent\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "324c49a2-ddaa-421d-9293-06a69d164edf",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Training gets complicated\n",
    "\n",
    "> In many cases, our optimization problem is **non-convex**, so it's more challenging to find a *global minimum*.\n",
    "\n",
    "- Analogy: rolling down a hill to reach the bottom, but you get stuck in a crevasse. \n",
    "- In these cases, researchers use techniques like **stochastic gradient descent** to improve optimization.\n",
    "\n",
    "<img src=\"img/networks/optimization.webp\" width=\"400\" alt=\"Optimization problem\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5e099e2-af44-4e85-8d09-ee8bfd9c1c95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lecture wrap-up\n",
    "\n",
    "- Neural networks are predictive models consisting of multiple **layers**, with **weights** connecting those layers.\n",
    "- **Training** a neural network consists of updating those weights (**parameters**), based on training data.\n",
    "- In general, weights are updated to *minimize prediction error*; this is called **gradient descent**.\n",
    "\n",
    "> Okay, so how can we actually *use* this systems?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
